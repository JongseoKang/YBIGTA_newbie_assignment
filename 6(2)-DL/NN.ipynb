{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Instructions:*\n",
    "- **과제 명세서를 읽어주시고 코드 작성을 해주시길 바랍니다**</span> \n",
    "- **명시된 step을 따라가며 전체적인 학습 방법을 숙지합니다**</span>\n",
    "- (**첫 번째 cell 결과로 나온 시간을 기준으로 채점을 하겠습니다**</span>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This code is written at 2025-01-27 16:46:25.024454\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "print(\"This code is written at \" + str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem\n",
    "\n",
    "Mutilayer Perceptron(```class MutiLayerPerceptron```)으로 간단한 Binary classification task를 진행해볼 것입니다. \n",
    "\n",
    "> 1. **Dataset**\n",
    ">> $\\texttt{moon}$ dataset\n",
    "> 2. **Network architecture**\n",
    "\n",
    " > $H_1 = X \\cdot W_1 + b_1$   \n",
    " > $z_1 = ReLU(H_1)$ where $ReLU$($=\\max(0,x)$) is a rectified linear unit and $z_1$ is an output of the first hidden layer.  \n",
    " > $H_2 = z_1 \\cdot W_2 + b_2$   \n",
    " > $z_2 = LeakyReLU(H_2)$ where $LeakyReLU$($=\\max(0.01x,x)$) and $z_2$ is an output of the second hidden layer. \n",
    " > $H_3 = z_2 \\cdot W_3 + b_3$   \n",
    " > $z_3 = tanh(H_3 + H_1)$ where $\\tanh$ is a tanh function and $z_3$ is an output of the third hidden layer.  \n",
    " > $H_4 = z_3 \\cdot W_4 + b_4$   \n",
    " > $\\hat y = \\sigma(H_4)$ where $\\sigma$ is a sigmoid function unit and $\\hat y$ is an output of the network.\n",
    " \n",
    " > **$W$** and **$b$**는 각각 weights와 bias.    \n",
    " > **weight 초기화**: Standard normal ($\\texttt{np.random.randn}$. 사용)  \n",
    " > **bias 초기화(intercept)**: 0     \n",
    " > **Input size**: 2  \n",
    " > **The first hidden layer size**: 10  \n",
    " > **The second hidden layer size**: 10  \n",
    " > **Output size**: 1   \n",
    " > **Regularization parameter $\\lambda$**: 0.001  \n",
    " > **Loss function**: Binary cross entropy loss (or equivently log loss).  \n",
    " > **Total loss** : \n",
    " > $L_{total} = \\sum_{i=1}^N{ (-y^{(i)}\\log \\hat{y}^{(i)} -(1-y^{(i)})\\log(1-\\hat{y}^{(i)})) } +  \\lambda \\|W\\|^2 $   \n",
    " > **Optimization**: Gradient descent  \n",
    " > **Learning rate** = 0.0001  \n",
    " > **Number of epochs** = 50000  \n",
    " > $y$는 정답, $\\hat{y}$는 예측값이고 0부터 1사이에 존재한다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmlp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultiLayerPerceptron\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets\n",
    "\n",
    "from mlp import MultiLayerPerceptron\n",
    "import utils\n",
    "\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "plt.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"STEP 1: Load data\")\n",
    "\n",
    "# Load data\n",
    "X_train, y_train = sklearn.datasets.make_moons(300, noise = 0.25)\n",
    "\n",
    "# Visualize data\n",
    "plt.scatter(X_train[:,0], X_train[:,1], s = 40, c=y_train, cmap=plt.cm.RdYlGn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"STEP 2: Train the model\")\n",
    "# random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# Hyperparameters\n",
    "nn_input_dim = 2\n",
    "nn_output_dim = 1\n",
    "nn_hdim1 = 10\n",
    "nn_hdim2 = 10\n",
    "nn_hdim3 = 10\n",
    "lr = 0.0001 \n",
    "L2_norm = 0.001\n",
    "epoch = 50000\n",
    "\n",
    "model = MultiLayerPerceptron(nn_input_dim, nn_hdim1, nn_hdim2, nn_hdim3, nn_output_dim, init=\"random\")\n",
    "stats = model.train(X_train, y_train, learning_rate=lr, L2_norm=L2_norm, epoch=epoch, print_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"STEP 3: Plot decision boundary\")\n",
    "# Plot the decision boundary\n",
    "utils.plot_decision_boundary(lambda x: model.predict(x), X_train, y_train)\n",
    "plt.title(f\"Decision Boundary: Hidden layer dimension {nn_hdim1, nn_hdim2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(np.arange(len(stats['loss_history'])) * 1000, stats['loss_history'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training loss over epoch')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(np.arange(len(stats['train_acc_history'])) * 1000, stats['train_acc_history'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Clasification accuracy')\n",
    "plt.title('Training accuracy over epoch')\n",
    "plt.gcf().set_size_inches(20, 8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
